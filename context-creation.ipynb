{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":6146260,"sourceType":"datasetVersion","datasetId":3521629}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/marcellemmer/context-creation?scriptVersionId=153621632\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Context creation using Wikipedia\n\nWe downloaded the 13GB Wikipedia Plaintext (2023-07-01) dataset from Kaggle. This dataset contains all the wikipedia articles up to the mentioned date stored in parquet files. We use only the wiki_2023_index.parquet file that contains the first sentences of the articles as context for our model. We assing a context column for each question of the Q&A dataframe from this file. This is done with the Sentence Transformer library that embeds the wikipedia articles and with Faiss that does a similarity search between the question and the first sentences of the articles. We retrieve the most similar wikipedia article for each question.\n\nThe code uses 2xT4 GPU from Kaggle\n\n## Sources\n\n* https://www.kaggle.com/datasets/jjinho/wikipedia-20230701/data?select=h.parquet\n\n* https://github.com/facebookresearch/faiss/wiki","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install datasets\n!pip install faiss-gpu sentence-transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing the libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nfrom datasets import load_dataset, Dataset\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom tqdm import tqdm\n\n# Set CUDA visible devices to GPU 0 and 1 (The 2xT4 GPUs that Kaggle provides)\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n\n# Important parameters describing the code\nSIM_MODEL = 'all-MiniLM-L6-v2'\n# Set device\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n# Define batch size\nBATCH_SIZE = 500_000\n\n#Loading the questions\nqna_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\",index_col=0)\n\nqna_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One possible source of context is to only use the 'wiki_2023_index.parquet' file that has the first sentences of each artice. This is what we considered first but most of the sentences were cut in the middle, providing little to no useful information about the topic.","metadata":{}},{"cell_type":"code","source":"# Load Parquet files into a Hugging Face dataset\n# Source: https://www.kaggle.com/datasets/jjinho/wikipedia-20230701/data?select=wiki_2023_index.parquet\n# You could use split='train[:1000] for testing is the files wrk properly\nwiki_dataset = load_dataset('parquet',\n                            data_files={'train': \"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\"},\n                            split='train[:4000]') # 1.76GB file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another possibility is to use every other file containing the full articles, but only load in the first two sentences. ","metadata":{}},{"cell_type":"code","source":"# Specify the folder containing the Parquet files\nfolder_path = \"/kaggle/input/wikipedia-20230701/\"\n\n# Get a list of all Parquet files in the folder\nparquet_files = [file for file in os.listdir(folder_path) \n                 if file.endswith(\".parquet\") \n                 and file != \"wiki_2023_index.parquet\"\n                and file != \"number.parquet\"\n                and file != \"other.parquet\"]\n\n# Initialize an empty list to store the selected text from each Parquet file\nselected_text = []\n\n# Function to extract the first two sentences from a text\ndef extract_first_two_sentences(text):\n    sentences = text.split('. ')[:2]\n    return '. '.join(sentences)\n\n# Iterate over each Parquet file, read only the 'text' column, extract the first two sentences, and append to the list\nfor parquet_file in tqdm(parquet_files, desc=\"Loading and Processing Parquet Files\", unit=\"file\"):\n    file_path = os.path.join(folder_path, parquet_file)\n    df = pq.read_table(file_path, columns=['text']).to_pandas()\n    df['context'] = df['text'].apply(extract_first_two_sentences)\n    selected_text.append(df['context'])\n\n# Concatenate all selected texts into a single Series\ncombined_selected_text = pd.DataFrame(pd.concat(selected_text, ignore_index=True))\n\n# Display the combined selected text Series\nprint(combined_selected_text.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the pandas dataframe into a huggingface dataset\nwiki_dataset = Dataset.from_pandas(combined_selected_text)\n# Free up memory\ndel selected_text, combined_selected_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wiki_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained sentence transformer model\nmodel = SentenceTransformer(SIM_MODEL)\n\n# Create a Faiss index on disk\nindex_path = \"/kaggle/working/faiss_index\"\ndimension = model.get_sentence_embedding_dimension()\n\n# Define parameters for IVF index\nnlist = 100  # Number of clusters (adjust as needed)\nquantizer = faiss.IndexFlatL2(dimension)\nindex = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_L2)\n\n# Iterate over the dataset in batches\nfor i in range(0, len(wiki_dataset['context']), BATCH_SIZE):\n    # Encode the context sentences using the SentenceTransformer model\n    context_embeddings = model.encode(wiki_dataset['context'][i:i+BATCH_SIZE],\n                                      device=DEVICE,\n                                      show_progress_bar=True,\n                                      convert_to_tensor=True,\n                                      normalize_embeddings=True).half()  # Use mixed-precision training (FP16) to reduce memory footprint\n\n    # Convert the embeddings to a numpy array\n    context_embeddings_np = context_embeddings.detach().cpu().numpy().astype('float32')\n\n    # Add the embeddings to the Faiss index\n    index.train(context_embeddings_np)\n    index.add_with_ids(context_embeddings_np, np.arange(0, context_embeddings_np.shape[0]))\n\n    # Wrap the index with IndexIDMap\n    index_ivf = faiss.IndexIDMap(index)\n\n    # Save the index\n    faiss.write_index(index_ivf.index, index_path)\n\n    # Free up memory\n    del context_embeddings, context_embeddings_np\n    \n    \n# Function to retrieve most similar documents\ndef retrieve_most_similar(query, k=20):\n    query_embedding = model.encode(query, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n    query_embedding = query_embedding.reshape(1, -1)  # Reshape for Faiss\n    query_embedding = query_embedding.detach().cpu().numpy()\n    _, idx = index.search(query_embedding, k)\n    return idx[0]\n\n# Example usage\nquery_text = qna_df['prompt'][0]\nprint(f'example prompt {query_text}')\nsimilar_documents_indices = retrieve_most_similar(query_text)\n\n# Print similar documents\nfor idx in similar_documents_indices:\n    print(wiki_dataset[int(idx)]['context'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the context column from the wikipedia article\n# Create an empty list to store the context for each prompt\ncontext_list = []\n\n# Loop through each prompt in the qna_df dataframe\nfor i in range(len(qna_df)):\n    query_text = qna_df['prompt'][i]\n    similar_documents_indices = retrieve_most_similar(query_text)\n\n    # Get the first answer from the corresponding wiki_dataset\n    context = wiki_dataset[int(similar_documents_indices[0])]['context']\n\n\n    context_list.append(context)\n\n# Add the context_list as a new column \"context\" to the qna_df DataFrame\nqna_df['context'] = context_list\n\n# Save the Q&A DataFrame to a CSV file\nqna_df.to_csv('openbook-qna-data.csv', index=False)\n\n# Display the modified DataFrame\nqna_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Faiss index to a file\nindex_file_path = \"wiki_faiss_index.index\"  # Specify the path where you want to save the index\nfaiss.write_index(index, index_file_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}